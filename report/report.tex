\documentclass[conference]{IEEEtran}
\usepackage{cite}
\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
\else
  \usepackage[dvips]{graphicx}
\fi
\usepackage{amsmath}
\usepackage{array}
\hyphenation{op-tical net-works semi-conduc-tor}
\makeatletter  
\newif\if@restonecol  
\makeatother  
\let\algorithm\relax  
\let\endalgorithm\relax  
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}%[ruled,vlined]{  
\usepackage{algpseudocode}  
\usepackage{amsmath}  
\renewcommand{\algorithmicrequire}{\textbf{Input:}}  % Use Input in the format of Algorithm  
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm   
\begin{document}
\title{Life is short, use everything2vec}
\author{\IEEEauthorblockN{Yikai Wang}
\IEEEauthorblockA{School of Data Science\\
Fudan University\\
15300180076@fudan.edu.cn}
\and
\IEEEauthorblockN{Dan Wu}
\IEEEauthorblockA{}
\and
\IEEEauthorblockN{Zheng Wei}
\IEEEauthorblockA{}}
\maketitle
\IEEEpeerreviewmaketitle
\section{Task Review}
\subsection{Task 1}
Design clustering algorithms or community mining algorithms to cluster all the papers in the data set. Use visualize tools to show all fields (ie, communities, identify corresponding community research topics), and highlight the most influential scholars in each field.
\subsection{Task 2}
Realization of demonstrating the ego-network to any input scholar (refer to the function example on the ArnetMiner website).
\subsection{Task 3}
Use the data provided by DBLP and ArnetMiner to analyze and model more social relationships among scholars, such as predicting the cooperation or citation relationship between two scholars, and predicting which conference will a scholar publish papers on in the future.
\section{division of work}
\begin{enumerate}
\item Yikai Wang: 

In brief, he applies several methods in network representation learning and uses and expands a hierarchical representation learning algorithm(HARP) for Networks. For homogeneous networks, he mainly uses deepwalk, node2vec and LINE. For heterogeneous networks, he uses metapath2vec++. For task 1, he uses both homogeneous NRL and heterogeneous NRL methods to extract features for clustering and uses KMeans and Birch to complete the clustering task. For task 3, he uses heterogeneous NRL methods to generate the vector representation of scholar cooperation network, scholar citation network and scholar-conference network.
\item Dan Wu:
\item Zheng Wei:
\end{enumerate}
\section{methodology}
\subsection{Network Representation Learning}
\begin{figure}[h]
\centering
\includegraphics[width=2.5in]{NRL}
\caption{Network representation learning flowchart}
\label{fig_NRL}
\end{figure}
Network is an important form of expressing the relationship between objects and objects. A key issue for the analysis of networks is to study how to reasonably represent feature information in the network. With the development of machine learning technology, feature learning for nodes in the network has become an emerging research task. Network representation learning algorithm transforms network information into low-dimensional dense real vector. 

There are two types of networks in NRL, one supposes that all nodes in the network have the same type, and the other supposes that all nodes have different types. The former network is called homogeneous network and the other is called heterogeneous network.

Formally, let $G=(V,E)$ be a graph, where $V$ is the set of nodes and $E$ is the set of edges. The goal of network representation learning is to develop a mapping function $\Phi:V\rightarrow R^{|V|*d},d\ll|V|$, which defines the latent representation of each node $v\in V$. Here we briefly introduce some homogeneous network learning methods and detailed introduce a heterogeneous network learning method and an optimization algorithm on all these methods.
\subsection*{Methods for homogeneous network learning}
\begin{enumerate}
	\item DeepWalk:
	
	The idea of DeepWalk\cite{Perozzi:2014:DOL:2623330.2623732} is borrowed from word2vec. It uses short random walks to generate paths of a node, then just like word2vec, it uses the path to generate a probability of its neighbors, which could give each node a vertices in networks.
		\item Node2vec:
		
		Node2vec\cite{DBLP:journals/corr/GroverL16} is an improvement for DeepWalk. In short, it regards the random walk as a searching problem. It provides a way of balancing the exploration-exploitation tradeoff that in turn leads to representations obeying a spectrum of equivalences from homophile to structural equivalence.
	\item LINE:
	
	LINE\cite{tang2015line} uses another way to realize network embedding. The main idea of this method is learning a low-dimensional embedding with preserving both the first-order proximity and the second-order proximity between the vertices.
\end{enumerate}
\subsection*{Methods for heterogeneous network learning}
Different with homogeneous networks, heterogeneous networks involve more than one node types and relationships between the same type of nodes and/or different types of nodes. Thereafter, these networks cannot be handled by representation learning models that specifically designed for homogeneous networks. In this project, we use a heterogeneous skip-gram model, \emph{metapath2vec++}\cite{dong2017metapath2vec}, to model the heterogeneous neighborhood of a node.

Specifically, in \emph{metapath2vec++}, we enable skip-gram to learn effective node representations for a heterogeneous network $G=(V,E,T)$ with $|T_V|>1$ by maximizing the probability of having the heterogeneous context $N_t(v),t\in T_V$ given a node $v$:
\begin{equation}
	arg max_{\theta}\sum_{v\in V}\sum_{t\in T_V}\sum_{c_t\in N_t(v)} log p(c_t|v;\theta)
\label{equ:argmax}
\end{equation}
where $N_t(v)$ denotes $v$'s neighborhood with the $t^{th}$ types of nodes and $p(c_t|v;\theta)$ is commonly defined as a softmax function, that is: $p(c_t|v;\theta)=\frac{exp\{X_{c_t}*X_v\}}{\sum_{u_t\in V_t}exp\{X_{u_t}*X_v\}}$, where $X_v$ is the $v^{th}$ row of $X$, representing the embedding vector for node $v$. $V_t$ is the node set of type $t$ in the network.

However, the method will cost a terrible long time. To solve the problem, negative sampling was introduced by Mikolov et al\cite{mikolov2013distributed}. Using this method, we just need to sample a small set of nodes from the network in order to construct softmax. Specifically, given a negative sample size $M$, we use following method to update Equation \ref{equ:argmax}:
\begin{equation}
log \sigma(X_{c_t}*X_v)+\sum_{m=1}^M E_{u_t^m\sim P_t(u_t)}[log \sigma(-X_{u_t^M}*X_v)]
\label{equ:OX}
\end{equation}
where $\sigma(x)=\frac{1}{1+e^{-x}}$ and $P(u)$ is the pre-defined distribution from which a negative node $u^m$ is drew from for $M$ times. In our model, we regard different types of nodes homogeneously and do not distinguish them when drawing negative nodes. The gradients of equation \ref{equ:OX} are derived as follows:
\begin{equation}\label{equ:gradient}
\begin{split}
\frac{\partial O(X)}{\partial X_{u_t^m}}=(\sigma(X_{u_t^m}X_v-I_{c_t}[u_t^m]))X_v\\
\frac{\partial O(X)}{\partial X_v}=\sum_{m=0}^M(\sigma(X_{u_t^m}X_v-I_{c_t}[u_t^m]))X_{u_t^m}
\end{split}
\end{equation}

Having defined the skip-gram model, the problem we need to solve is how to effectively transform the heterogeneous network into skip-gram. Similar to homogeneous network, we can use random walk to generate paths of multiple types of nodes. Different with homogeneous network, in heterogeneous network, we need to consider the type of the nodes in the random walk.

Formally, a meta-path scheme $P$ is defined as a path that is denoted in the form of $V_1 \xrightarrow{R_1}V_2\xrightarrow{R_2}\dots V_t\xrightarrow{R_t}V_{t+1}\dots\xrightarrow{R_{l-1}}V_l$, wherein $R=R_1 \circ R_2 \circ \dots \circ R_{l-1}$ defines the composite relations between node types $V_1$ and $V_l$. Thus we could show how to use meta-paths to guide heterogeneous random walkers. Given a heterogeneous network $G=(V,E,T)$ and a meta-path scheme $P$, the transition probability at step $i$ is as follows:
\begin{equation}
	p(v^{i+1}|v_t^i,P)=\begin{cases}
		\frac{1}{|N_{t+1}(v_t^l)|}&(v^{i+1},v_t^i)\in E,\phi(v^{i+1})=t+1\\0&(v^{i+1},v_t^i)\in E,\phi(v^{i+1})\neq t+1\\0&(v^{i+1},v_t^i)\notin E
	\end{cases}
\label{equ:tran}
\end{equation}
where $v_t^i\in V_t$ and $N_{t+1}(v_t^i)$ denote the $V_{t+1}$ type of neighborhood of node $v_t^i$. Further more, meta-paths are commonly used in a symmetric way, which means its first node type is the same as the last one. That is:
\begin{equation}
	p(v^{i+1}|v_t^i)=p(v^{i+1}|v_1^i),\text{if } t=l
\end{equation}
The complete algorithm is expressed as follows:
\begin{algorithm}[h]
  \caption{The metapath2vec++ Algorithm.}  
  \KwIn{The heterogeneous information network $G=(V,E,T)$, a meta-path scheme $P$,\# walks per node $w$, walk length $l$, embedding dimension $d$,neighborhood size $k$}  
  \KwOut{The latent node embeddings $X\in R^{|V|*d}$}  
  initialize $X$\;  
  \For{$i=1;i \le w$}  
  {\For{$v\in V$}{
  MP=MetaPathRandomWalk($G,P,v,l$)\;
  X=HeterogeneousSkipGram($X,k,MP$)\;}
  }
  return $X$\;
  
  \textbf{MetaPathRandomWalk}($G,P,v,l$)
  MP[1]=v\;
  \For{$i=1;i<l$}{
  draw u according to Eq. \ref{equ:tran}\;
  MP[i+1]=u\;}
  return MP\;
  
  \textbf{HeterogneousSkipGram}($X,k,MP$)
  \For{$i=1;i\le l$}{
  v = MP[i]\;
  \For{$j=max(0,i-k);j\le min(i+k,l);j\neq i$}{
  $c_t=MP[j]$\;
  $X^{new}=X^{old}-\eta \frac{\partial O(X)}{\partial X}$(Eq. \ref{equ:gradient}\;}}
\end{algorithm}  
\subsection*{HARP}
The network embedding method we introduced above is very useful in some situations.
 However, there are two main disadvantages for these methods:
\begin{enumerate}
	\item all the models do not involve high-order network structural information
	\item their stochastic optimization can fall victim to poor initialization
\end{enumerate}
Thus, we change the traditional problem into hierarchical representation learning problem. The main idea is we seek to find a graph $G_s=(V_s,E_s)$ which captures the essential structure of $G$, but is much smaller(i.e. $|V_s|\ll|V|,|E_s|\ll|E|$). It is trivial that $G_s$ is easier to embed. Since we have much less relationships, which means the mapping could be much smoother. Further more, since the $G_s$ is much smaller, the models that focus on local structure now could have a better performance on global structure's representation.

Our method for multilevel network representation learning, HARP\cite{chen2017harp}, consists of three parts-graph coarsening, graph embedding and representation refinement.

\begin{enumerate}
	\item Graph Coarsening:
	
	Given a graph $G$, graph coarsening algorithms create a hierarchy of successively smaller graphs $G_0,G_1,\ldots,G_L$, where $G_0=G$. The coarser graphs preserve the global structure of the original graph and have much fewer nodes and edges.
	\item Graph Embedding on the Coarsest Graph:
	
	Using provided network embedding algorithms to to graph embedding.(DeepWalk, Node2vec, LINE, etc.)
	\item Graph Representation Prolongation and Refinement:
	
	We prolong and refine the graph representation from the coarsest to the finest graph. For each graph $G_i$, we use the graph representation of $G_{i+1}$ as its initial embedding and refine the graph embedding.
\end{enumerate}
The complete algorithm is expressed as follows:
\begin{algorithm}[h]
  \caption{HARP(G,Embed())}  
  \KwIn{network $G=(V,E)$, arbitrary network embedding algorithm EMBED()}  
  \KwOut{ node embeddings $\Phi\in R^{|V|*d}$}  
  $G_0,G_1,\ldots,G_L\leftarrow$GRAPHCOARSENING(G)\;  
  Initial $\Phi_{G_L}^{'}$ by assigning zeros\;
  $\Phi_{G_L}\leftarrow EMBED(G_L,\Phi_{G_L}^{'})$
  \For{$i=L-1;i \geq 0$}  
  {$\Phi_{G_i}^{'}\leftarrow PROLONGATE(\Phi_{G_{i+1}},G_{i+1},G_i)$\;
  $\Phi_{G_i}\leftarrow EMBED(G_i,\Phi_{G_i}^{'})$
  }
  return $\Phi_{G_0}$\;
  
  \textbf{GraphGoarsening}($G(V,E)$)
  $L\leftarrow 0$\;
  $G_0\leftarrow G$\;
  \While{$|V_L|\geq threshold$}{
  $L\leftarrow L+1$\;
  $G_L\leftarrow$EDGECOLLAPSE(STARCOLLAPSE(G))\;}
  return $G_0,G_1,\ldots,G_L$\;
\end{algorithm}  

In the algorithm, \textbf{Edge Collapse} is an algorithm for preserving first-order proximity when coarsening the network. For edges $E$, it will select $E^{'}\subset E$, satisfying no two edges in the subset are incident to the same vertex. That is, for each $(u_i,v_i)\in E^{'}$, it merges the pairs into a single node $w_i$, and merge the edges incident to $u_i$ and $v_i$. \textbf{Star Collapse} is an algorithm for preserving second-order proximity when coarsening the network. It will merges nodes with the same neighbors into super-nodes. For example, in the Figure \ref{fig_HARP},$(v_1,v_2),(v_3,v_4)$ and $(v_5,v_6)$ are merged into super-nodes as they share the same neighbors $(v_7)$.
\begin{figure}[h]
\centering
\includegraphics[width=2.5in]{HARP}
\caption{Illustration of graph coarsening algorithms}
\label{fig_HARP}
\end{figure}

It is worth mentioning that in the model of HARP, they just consider homogeneous networks. However, without much modify, it can be used in heterogeneous networks, too. Specifically, in the algorithm, we first do star collapse and then do edge collapse. For a heterogeneous network, since the edges are mostly between two different types of nodes, we could find that similar nodes with the same type will be regard as the same super-node, which is just what we hope to happen.
\section{Experiment}
\subsection{Data Set}
Our data are selected from Arnetminer\cite{tang2008arnetminer}. We choose all papers published on 21 conferences until 2017. A summary of our data are shown in Table \ref{data}:
\begin{table*}[h]
\center
	\begin{tabular}{c|c|c|c}
	\hline
	\textbf{Field} & \textbf{Conferences} & \textbf{Number of Papers}&\textbf{Number of scholars}\\
	\hline
	Distributed \& Parallel Computing & PPOPP,PACT,IPDPS,ICPP & Number of Papers&Number of scholars\\
	\hline
	Natural Language Processing & ACL,EACL,COLING, EMNLP &&\\
	\hline
	Data Mining & ICDE,SIGMOD,KDD,ICDM&&\\
	\hline
	Computer Education & AIED,ITS,ICALT&&\\
	\hline
	Machine Learning &IJCAI,ICML,NIPS&&\\
	\hline
	Operating Systems \& Simulations & MASCOTS,SOSP,OSDI&&\\
	\hline
	\end{tabular}
	\caption{A summary of data.}
	\label{data}
\end{table*}
\subsection{Task 1}
The first part of task 1 is clustering. We need to cluster all the papers with the information involving authors' names, references, title, abstract and year. We tried several methods to make use of the information. Specifically, word2vec uses title and abstract to generate a vector. DeepWalk and LINE uses the citation relationship of papers. Metapath2vec++ regards papers and authors as node and generate vectors. The accuracy of our methods are shows in Table \ref{task1cluster}. From the result we could find that the information between papers and authors are more important the relationship between papers and papers. Further, our HARP model could improve the accuracy of about 5 percent. It also shows that KMeans is better than Birch in some sense. And word2vec seems useless from our result.
\begin{table}[h]
\center
	\begin{tabular}{c|c|c}
	\hline
	\hline
	\textbf{Model/Cluster Algorithm}& KMeans & Birch\\
	\hline
	\hline
	word2vec&44.25\%&45.13\%\\
	\hline
	DeepWalk &53.08\%&53.85\%\\
	\hline
	LINE&50.44\%&38.82\%\\
	\hline
	metapath2vec++&66.39\%&59.69\%\\
	\hline
	HARP(metapath\&DeepWalk)&\textbf{71.66\%}&61.24\%\\
	\hline
	HARP(metapath\&line)&67.05\%&53.49\%\\
	\hline
	DeepWalk+word2vec&52.26\%&52.24\%\\
	\hline
	LINE+word2vec&51.21\%&36.77\%\\
	\hline
	(metapath2vec++)+word2vec&49.88\%&58.12\%\\
	\hline
	HARP(metapath\&DeepWalk)+word2vec&51.72\%&55.39\%\\
	\hline
	HARP(metapath\&line)+word2vec&60.55\%&53.15\%\\
	\hline
	\hline
	\end{tabular}	
	\caption{Accuracy of models in clustering task.}
	\label{task1cluster}
\end{table}
\subsection{Task 2}
\subsection{Task 3}
For this task, we construct several networks. A trivial idea is that we use the whole network, which involves edges between papers and papers(reference), papers and authors, papers and conferences. Another idea is just consider the relationship we want to survey. In the cooperation task, we just construct a network which involves edges between papers and authors. In the citation task, we just construct a network which involves edges between papers and papers, papers and authors. And in publish task, we just construct a network with edges involve relationship between authors and conferences.

To quantify the accuracy of prediction algorithms. We use a method called area under the receiver operating characteristic curve(AUC)\cite{lu2011link}. 
\subsubsection*{Definition of AUC}
Provided the rank of all non-observed links, the AUC values can be interpreted as the probability that a randomly chosen missing link is given a higher score than a randomly chosen nonexistent link. To save time, we calculate the score of each non-observed link instead of giving the ordered list. At each time, we randomly pick a missing link and a nonexistent link to compare their scores. If among n independent comparisons, there are $n_1$ times the missing link having a higher score and $n_2$ times they have the same score, the AUC value is:
\begin{equation}
AUC=\frac{n_1+0.5n_2}{n}
\end{equation}
If all the scores are generated from an independent and identical distribution, the AUC value should be about 0.5. Thereafter, the degree to which the value exceeds 0.5 indicates how much better the algorithm performs than pure chance.

In our experiments, we use the cosine distance between two vectors as the score of two nodes. The result is shown in Table \ref{AUC}
\begin{table}[h]
\center
\begin{tabular}{c|c|c|c}
\hline
\hline
\textbf{Model/task} & \textbf{cooperation} & \textbf{citation} & \textbf{publish}\\
\hline
\hline
metapath2vec++ &75.39\%&66.92\%&77.86\%\\
\hline
HARP(DeepWalk)&83.38\%&\textbf{87.80 \%}&87.76\%\\
\hline
HARP(LINE)&83.73\%&76.93\%& 67.32\%\\
\hline
HARP(DeepWalk)(whole net)& \textbf{85.72\%}&85.30\%&\textbf{88.50\%}\\
\hline
HARP(LINE)(whole net)&56.86\%&56.43\%& 54.81\%\\
\hline
\hline
\end{tabular}
\caption{AUC of task3}
\label{AUC}
\end{table}
\section{Conclusion and Discussion}
\bibliographystyle{IEEEtran}
\bibliography{report}
\end{document}


