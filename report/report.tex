\documentclass[conference]{IEEEtran}
\usepackage{cite}
\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
\else
  \usepackage[dvips]{graphicx}
\fi
\usepackage{amsmath}
\usepackage{array}
\hyphenation{op-tical net-works semi-conduc-tor}
\makeatletter  
\newif\if@restonecol  
\makeatother  
\let\algorithm\relax  
\let\endalgorithm\relax  
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}%[ruled,vlined]{  
\usepackage{algpseudocode}  
\usepackage{amsmath}  
\renewcommand{\algorithmicrequire}{\textbf{Input:}}  % Use Input in the format of Algorithm  
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm   
\begin{document}
\title{Life is short, use everything2vec}
\author{\IEEEauthorblockN{Yikai Wang}
\IEEEauthorblockA{School of Data Science\\
Fudan University\\
15300180076@fudan.edu.cn}
\and
\IEEEauthorblockN{Dan Wu}
\IEEEauthorblockA{School of Data Science\\
Fudan University\\
15300180076@fudan.edu.cn}
\and
\IEEEauthorblockN{Zheng Wei}
\IEEEauthorblockA{School of Data Science\\
Fudan University\\
15300180076@fudan.edu.cn}}
\maketitle
\IEEEpeerreviewmaketitle
\section{Task Review}
\subsection{Task 1}
Design clustering algorithms or community mining algorithms to cluster all the papers in the data set. Use visualize tools to show all fields (ie, communities, identify corresponding community research topics), and highlight the most influential scholars in each field.
\subsection{Task 2}
Realization of demonstrating the ego-network to any input scholar (refer to the function example on the ArnetMiner website).
\subsection{Task 3}
Use the data provided by DBLP and ArnetMiner to analyze and model more social relationships among scholars, such as predicting the cooperation or citation relationship between two scholars, and predicting which conference will a scholar publish papers on in the future.
\section{division of work}
\begin{enumerate}
\item Yikai Wang: 

In brief, he applies several methods in network representation learning and uses and expands a hierarchical representation learning algorithm(HARP) for Networks. For homogeneous networks, he mainly uses deepwalk, node2vec and LINE. For heterogeneous networks, he uses metapath2vec++. For task 1, he uses both homogeneous NRL and heterogeneous NRL methods to extract features for clustering and uses KMeans and Birch to complete the clustering task. For task 3, he uses heterogeneous NRL methods to generate the vector representation of scholar cooperation network, scholar citation network and scholar-conference network.
\item Dan Wu:
\begin{enumerate}
\item Revise the preprocessing part of word2vec model in task 1. (Stop words deleted, all words are transformed into lower case.)
\item Build a LDA model to get the key words of each group of the papers in task 1.
\item Use PageRank index to calculate the influence of each paper and scholar. Visualize  the citation and cooperative relationship in task 1.
\item Visualize the ego-network in task 2.
\item Build a baseline model for link prediction in task 3.
\end{enumerate}
\item Zheng Wei:
\end{enumerate}
\section{methodology}
\subsection{Network Representation Learning}
\begin{figure}[h]
\centering
\includegraphics[width=2.5in]{NRL}
\caption{Network representation learning flowchart}
\label{fig_NRL}
\end{figure}
Network is an important form of expressing the relationship between objects and objects. A key issue for the analysis of networks is to study how to reasonably represent feature information in the network. With the development of machine learning technology, feature learning for nodes in the network has become an emerging research task. Network representation learning algorithm transforms network information into low-dimensional dense real vector. 

There are two types of networks in NRL, one supposes that all nodes in the network have the same type, and the other supposes that all nodes have different types. The former network is called homogeneous network and the other is called heterogeneous network.

Formally, let $G=(V,E)$ be a graph, where $V$ is the set of nodes and $E$ is the set of edges. The goal of network representation learning is to develop a mapping function $\Phi:V\rightarrow R^{|V|*d},d\ll|V|$, which defines the latent representation of each node $v\in V$. Here we briefly introduce some homogeneous network learning methods and detailed introduce a heterogeneous network learning method and an optimization algorithm on all these methods.
\subsection*{Methods for homogeneous network learning}
\begin{enumerate}
	\item DeepWalk:
	
	The idea of DeepWalk\cite{Perozzi:2014:DOL:2623330.2623732} is borrowed from word2vec. It uses short random walks to generate paths of a node, then just like word2vec, it uses the path to generate a probability of its neighbors, which could give each node a vertices in networks.
		\item Node2vec:
		
		Node2vec\cite{DBLP:journals/corr/GroverL16} is an improvement for DeepWalk. In short, it regards the random walk as a searching problem. It provides a way of balancing the exploration-exploitation tradeoff that in turn leads to representations obeying a spectrum of equivalences from homophile to structural equivalence.
	\item LINE:
	
	LINE\cite{tang2015line} uses another way to realize network embedding. The main idea of this method is learning a low-dimensional embedding with preserving both the first-order proximity and the second-order proximity between the vertices.
\end{enumerate}
\subsection*{Methods for heterogeneous network learning}
Different with homogeneous networks, heterogeneous networks involve more than one node types and relationships between the same type of nodes and/or different types of nodes. Thereafter, these networks cannot be handled by representation learning models that specifically designed for homogeneous networks. In this project, we use a heterogeneous skip-gram model, \emph{metapath2vec++}\cite{dong2017metapath2vec}, to model the heterogeneous neighborhood of a node.

Specifically, in \emph{metapath2vec++}, we enable skip-gram to learn effective node representations for a heterogeneous network $G=(V,E,T)$ with $|T_V|>1$ by maximizing the probability of having the heterogeneous context $N_t(v),t\in T_V$ given a node $v$:
\begin{equation}
	arg max_{\theta}\sum_{v\in V}\sum_{t\in T_V}\sum_{c_t\in N_t(v)} log p(c_t|v;\theta)
\label{equ:argmax}
\end{equation}
where $N_t(v)$ denotes $v$'s neighborhood with the $t^{th}$ types of nodes and $p(c_t|v;\theta)$ is commonly defined as a softmax function, that is: $p(c_t|v;\theta)=\frac{exp\{X_{c_t}*X_v\}}{\sum_{u_t\in V_t}exp\{X_{u_t}*X_v\}}$, where $X_v$ is the $v^{th}$ row of $X$, representing the embedding vector for node $v$. $V_t$ is the node set of type $t$ in the network.

However, the method will cost a terrible long time. To solve the problem, negative sampling was introduced by Mikolov et al\cite{mikolov2013distributed}. Using this method, we just need to sample a small set of nodes from the network in order to construct softmax. Specifically, given a negative sample size $M$, we use following method to update Equation \ref{equ:argmax}:
\begin{equation}
log \sigma(X_{c_t}*X_v)+\sum_{m=1}^M E_{u_t^m\sim P_t(u_t)}[log \sigma(-X_{u_t^M}*X_v)]
\label{equ:OX}
\end{equation}
where $\sigma(x)=\frac{1}{1+e^{-x}}$ and $P(u)$ is the pre-defined distribution from which a negative node $u^m$ is drew from for $M$ times. In our model, we regard different types of nodes homogeneously and do not distinguish them when drawing negative nodes. The gradients of equation \ref{equ:OX} are derived as follows:
\begin{equation}\label{equ:gradient}
\begin{split}
\frac{\partial O(X)}{\partial X_{u_t^m}}=(\sigma(X_{u_t^m}X_v-I_{c_t}[u_t^m]))X_v\\
\frac{\partial O(X)}{\partial X_v}=\sum_{m=0}^M(\sigma(X_{u_t^m}X_v-I_{c_t}[u_t^m]))X_{u_t^m}
\end{split}
\end{equation}

Having defined the skip-gram model, the problem we need to solve is how to effectively transform the heterogeneous network into skip-gram. Similar to homogeneous network, we can use random walk to generate paths of multiple types of nodes. Different with homogeneous network, in heterogeneous network, we need to consider the type of the nodes in the random walk.

Formally, a meta-path scheme $P$ is defined as a path that is denoted in the form of $V_1 \xrightarrow{R_1}V_2\xrightarrow{R_2}\dots V_t\xrightarrow{R_t}V_{t+1}\dots\xrightarrow{R_{l-1}}V_l$, wherein $R=R_1 \circ R_2 \circ \dots \circ R_{l-1}$ defines the composite relations between node types $V_1$ and $V_l$. Thus we could show how to use meta-paths to guide heterogeneous random walkers. Given a heterogeneous network $G=(V,E,T)$ and a meta-path scheme $P$, the transition probability at step $i$ is as follows:
\begin{equation}
	p(v^{i+1}|v_t^i,P)=\begin{cases}
		\frac{1}{|N_{t+1}(v_t^l)|}&(v^{i+1},v_t^i)\in E,\phi(v^{i+1})=t+1\\0&(v^{i+1},v_t^i)\in E,\phi(v^{i+1})\neq t+1\\0&(v^{i+1},v_t^i)\notin E
	\end{cases}
\label{equ:tran}
\end{equation}
where $v_t^i\in V_t$ and $N_{t+1}(v_t^i)$ denote the $V_{t+1}$ type of neighborhood of node $v_t^i$. Further more, meta-paths are commonly used in a symmetric way, which means its first node type is the same as the last one. That is:
\begin{equation}
	p(v^{i+1}|v_t^i)=p(v^{i+1}|v_1^i),\text{if } t=l
\end{equation}
The complete algorithm is expressed as follows:
\begin{algorithm}[h]
  \caption{The metapath2vec++ Algorithm.}  
  \KwIn{The heterogeneous information network $G=(V,E,T)$, a meta-path scheme $P$,\# walks per node $w$, walk length $l$, embedding dimension $d$,neighborhood size $k$}  
  \KwOut{The latent node embeddings $X\in R^{|V|*d}$}  
  initialize $X$\;  
  \For{$i=1;i \le w$}  
  {\For{$v\in V$}{
  MP=MetaPathRandomWalk($G,P,v,l$)\;
  X=HeterogeneousSkipGram($X,k,MP$)\;}
  }
  return $X$\;
  
  \textbf{MetaPathRandomWalk}($G,P,v,l$)
  MP[1]=v\;
  \For{$i=1;i<l$}{
  draw u according to Eq. \ref{equ:tran}\;
  MP[i+1]=u\;}
  return MP\;
  
  \textbf{HeterogneousSkipGram}($X,k,MP$)
  \For{$i=1;i\le l$}{
  v = MP[i]\;
  \For{$j=max(0,i-k);j\le min(i+k,l);j\neq i$}{
  $c_t=MP[j]$\;
  $X^{new}=X^{old}-\eta \frac{\partial O(X)}{\partial X}$(Eq. \ref{equ:gradient}\;}}
\end{algorithm}  
\subsection*{HARP}
The network embedding method we introduced above is very useful in some situations.
 However, there are two main disadvantages for these methods:
\begin{enumerate}
	\item all the models do not involve high-order network structural information
	\item their stochastic optimization can fall victim to poor initialization
\end{enumerate}
Thus, we change the traditional problem into hierarchical representation learning problem. The main idea is we seek to find a graph $G_s=(V_s,E_s)$ which captures the essential structure of $G$, but is much smaller(i.e. $|V_s|\ll|V|,|E_s|\ll|E|$). It is trivial that $G_s$ is easier to embed. Since we have much less relationships, which means the mapping could be much smoother. Further more, since the $G_s$ is much smaller, the models that focus on local structure now could have a better performance on global structure's representation.

Our method for multilevel network representation learning, HARP\cite{chen2017harp}, consists of three parts-graph coarsening, graph embedding and representation refinement.

\begin{enumerate}
	\item Graph Coarsening:
	
	Given a graph $G$, graph coarsening algorithms create a hierarchy of successively smaller graphs $G_0,G_1,\ldots,G_L$, where $G_0=G$. The coarser graphs preserve the global structure of the original graph and have much fewer nodes and edges.
	\item Graph Embedding on the Coarsest Graph:
	
	Using provided network embedding algorithms to to graph embedding.(DeepWalk, Node2vec, LINE, etc.)
	\item Graph Representation Prolongation and Refinement:
	
	We prolong and refine the graph representation from the coarsest to the finest graph. For each graph $G_i$, we use the graph representation of $G_{i+1}$ as its initial embedding and refine the graph embedding.
\end{enumerate}
The complete algorithm is expressed as follows:
\begin{algorithm}[h]
  \caption{HARP(G,Embed())}  
  \KwIn{network $G=(V,E)$, arbitrary network embedding algorithm EMBED()}  
  \KwOut{ node embeddings $\Phi\in R^{|V|*d}$}  
  $G_0,G_1,\ldots,G_L\leftarrow$GRAPHCOARSENING(G)\;  
  Initial $\Phi_{G_L}^{'}$ by assigning zeros\;
  $\Phi_{G_L}\leftarrow EMBED(G_L,\Phi_{G_L}^{'})$
  \For{$i=L-1;i \geq 0$}  
  {$\Phi_{G_i}^{'}\leftarrow PROLONGATE(\Phi_{G_{i+1}},G_{i+1},G_i)$\;
  $\Phi_{G_i}\leftarrow EMBED(G_i,\Phi_{G_i}^{'})$
  }
  return $\Phi_{G_0}$\;
  
  \textbf{GraphGoarsening}($G(V,E)$)
  $L\leftarrow 0$\;
  $G_0\leftarrow G$\;
  \While{$|V_L|\geq threshold$}{
  $L\leftarrow L+1$\;
  $G_L\leftarrow$EDGECOLLAPSE(STARCOLLAPSE(G))\;}
  return $G_0,G_1,\ldots,G_L$\;
\end{algorithm}  

In the algorithm, \textbf{Edge Collapse} is an algorithm for preserving first-order proximity when coarsening the network. For edges $E$, it will select $E^{'}\subset E$, satisfying no two edges in the subset are incident to the same vertex. That is, for each $(u_i,v_i)\in E^{'}$, it merges the pairs into a single node $w_i$, and merge the edges incident to $u_i$ and $v_i$. \textbf{Star Collapse} is an algorithm for preserving second-order proximity when coarsening the network. It will merges nodes with the same neighbors into super-nodes. For example, in the Figure \ref{fig_HARP},$(v_1,v_2),(v_3,v_4)$ and $(v_5,v_6)$ are merged into super-nodes as they share the same neighbors $(v_7)$.
\begin{figure}[h]
\centering
\includegraphics[width=2.5in]{HARP}
\caption{Illustration of graph coarsening algorithms}
\label{fig_HARP}
\end{figure}

It is worth mentioning that in the model of HARP, they just consider homogeneous networks. However, without much modify, it can be used in heterogeneous networks, too. Specifically, in the algorithm, we first do star collapse and then do edge collapse. For a heterogeneous network, since the edges are mostly between two different types of nodes, we could find that similar nodes with the same type will be regard as the same super-node, which is just what we hope to happen.
\subsection{Pagerank}
There are ome examples of metrics used to evaluate the publication record of a scientist are the number of publications, total number of citations, the number of citations per paper, such as the i10-index and the h-index. The h-index is perhaps the most sophisticated and nuanced measure among these, since it accounts for both the quality and the quantity of a scientists’ research publications. However, many shortcomings of h-index have been pointed out creating considerable debate over the use of h-index \cite{costas2007h}\cite{egghe2006theory}\cite{zhivotovsky2008self}, and many variants of h-index have been proposed to address these. However, a fundamental issue not addressed by all these metrics is that they still treat all citations equally. Yet, it is perfectly clear that a citation by a paper from a highly regarded journal, such as Nature, should be treated differently from a citation by a workshop paper or a technical report. If this does not happen, locally famous authors whose research does not have global impact but gets cited by their colleagues in their country or research circle can get rewarded. 

Considering this problem, we use the pagerank-index to calculate the influence of papers and scholars, which is designed to address the drawbacks of existing indices by utilizing the underlying citation network dynamics. The index is calculated by running the well-known pagerank algorithm [4] on the evolving citation network to give scores to papers.  We use the original form of pagerank algorithm to come up with pagerank values for each publication, and the score of an author is calculated as a weighted sum of the scores of the papers he/she has written. The process has three stages: (i) computing the page-rank value of each paper in the system (each node in the citation network) (ii) assigning weighted proportions of such values to each author in the system (each node in the collaboration network) (iii) computing the author pagerank-index as a percentile. This process is illustrated by Figure \ref{fig_pagerank} .
\begin{figure}[h]
\centering
\includegraphics[width=2.5in]{pagerank}
\caption{The process of computing pagerank index.}
\label{fig_pagerank}	
\end{figure}
In the first stage, publications are ranked using the page-rank algorithm as described by Larry Page and Sergei Brin \cite{page1999pagerank} which can be interpreted to mimic the behavior of a random surfer in the world wide web. We use page-rank algorithm as described by Eqs \ref{equ_pagerank} where $P_i^t$ is the pagerank of node i at time t, $A_{ij}$ is the adjacency matrix of the network, $k_{out}(j)$ is the number of outgoing links from node *j *and $\alpha$ is a reset parameter. N is the number of nodes in the network.
\begin{equation}
	P_i^t = \frac{1-\alpha}{N}+ \alpha \sum_j \frac{A_{ij}P_j^{(t-1)}}{k_{out}(j)} 
\label{equ_pagerank}
\end{equation}
The second stage of computation involves distributing this page-rank value among the respective authors of each publication. The page-rank value of the publication could be distributed equally between the authors. However, we find this distribution to be typically flawed because equal weight would be given to the first author and the last author. However the usual practice is to order the authors by the contributions they made to a certain publication. Hence, in order to maintain fairness and objectivity, we distribute the page-rank values proportionately as shown in Eqs \ref{wooden_1} and \ref{wooden_2}, where $W_d^s$ is the weight of the pagerank value assigned to a particular author (scientist) s from document d, $N^d_a$  is the number of authors of document d, whereas $R_s$ is the ‘position’ of author (scientist) s in the list of authors in document d. Further,$\rho_d$ is the pagerank value of document (node) d at steady state, and$\rho_d^s$ is the pagerank value assigned to author s from it.
\begin{equation}
W_d^s = \frac{N_a-R_s +1}{0.5N_a(N_a+1)}
\label{wooden_1}
\end{equation}
\begin{equation}
	\rho_d^s = W_d^s \cdot \rho_d
\label{wooden_2}
\end{equation}
The final stage involves aggregating the page-rank values received by each author from each of their publications respectively, to come up with a single page-rank summation value for each author node in the collaboration network, as shown in Eqs \ref{wooden_3}.
\begin{equation}
\Omega_s =\sum_d \rho_d^s 
\label{wooden_3}
\end{equation}
\subsection{Networkx}
As a baseline model, We use preferential attachment model. Preferential attachment has received considerable attention as a model of the growth of net-works \cite{mitzenmacher2004brief}. The basic premise is that the probability that a new edge involves node x is proportional to $|\Gamma(x)|$, the current number of neighbors of x. Newman \cite{newman2001clustering} and Barabasi et al. \cite{barabasi2002evolution} have further proposed, on the basis of empirical evidence, that the probability of co-authorship of x and y is correlated with the product of the number of collaborators of x and y. This corresponds to the measure $score(x, y) := |\Gamma(x)|*|\Gamma(y)|$. 
\section{Experiment}
\subsection{Data Set}
Our data are selected from Arnetminer\cite{tang2008arnetminer}. We choose all papers published on 21 conferences until 2017. A summary of our data are shown in Table \ref{data}:
\begin{table*}[h]
\center
	\begin{tabular}{c|c|c|c}
	\hline
	\hline
	\textbf{Field} & \textbf{Conferences} & \textbf{Number of Papers}&\textbf{Number of scholars}\\
	\hline
	\hline
	Distributed \& Parallel Computing & PPOPP,PACT,IPDPS,ICPP & 12309&20135\\
	\hline
	Natural Language Processing & ACL,EACL,COLING, EMNLP &11894&12224\\
	\hline
	Data Mining & ICDE,SIGMOD,KDD,ICDM&18454&26174\\
	\hline
	Computer Education & AIED,ITS,ICALT&5624&8564\\
	\hline
	Machine Learning &IJCAI,ICML,NIPS&17200
&20320\\
	\hline
	Operating Systems \& Simulations & MASCOTS,SOSP,OSDI&2100&4502\\
	\hline
	\hline
	\end{tabular}
	\caption{A summary of data.}
	\label{data}
\end{table*}
\begin{table*}[h]
\center
	\begin{tabular}{c|c}	
	\hline
	\hline
  Real Subjects & Subjects Extraction\\
  \hline
  \hline
    Distributed\&Parallel System & \emph{performance}, \textbf{parallel,}
  \emph{memory}, \textbf{distributed}, applications\\
  \hline
  Natural Language Processing & \textbf{language}, based, \emph{model},
  \textbf{text}, using\\
  \hline
  Machine Learning & \textbf{learning}, \textbf{classification}, data,
  \emph{training, feature}\\
  \hline
  Data Mining & \emph{algorithm}, \emph{model}, time, problem,
  algorithms\\
  \hline
  Operating Systems / Simulations & based, \textbf{systems}, paper,
  user, information\\
  \hline
  Computer Education & data, query, mining, database,
  queries\\
  \hline
  \hline
  \end{tabular}
  
 \caption{The Subject Extraction using LDA model.}
 \label{LDA}
  \end{table*}

\subsection{Task 1}
\subsubsection{Clustering}
The first part of task 1 is clustering. We need to cluster all the papers with the information involving authors' names, references, title, abstract and year. We tried several methods to make use of the information. Specifically, word2vec uses title and abstract to generate a vector. DeepWalk and LINE uses the citation relationship of papers. Metapath2vec++ regards papers and authors as node and generate vectors. The accuracy of our methods are shows in Table \ref{task1cluster}. From the result we could find that the information between papers and authors are more important the relationship between papers and papers. Further, our HARP model could improve the accuracy of about 5 percent. It also shows that KMeans is better than Birch in some sense. And word2vec seems useless from our result.
\begin{table}[h]
\center
	\begin{tabular}{c|c|c}
	\hline
	\hline
	\textbf{Model/Cluster Algorithm}& KMeans & Birch\\
	\hline
	\hline
	word2vec&44.25\%&45.13\%\\
	\hline
	DeepWalk &53.08\%&53.85\%\\
	\hline
	LINE&50.44\%&38.82\%\\
	\hline
	metapath2vec++&66.39\%&59.69\%\\
	\hline
	HARP(metapath\&DeepWalk)&\textbf{71.66\%}&61.24\%\\
	\hline
	HARP(metapath\&line)&67.05\%&53.49\%\\
	\hline
	DeepWalk+word2vec&52.26\%&52.24\%\\
	\hline
	LINE+word2vec&51.21\%&36.77\%\\
	\hline
	(metapath2vec++)+word2vec&49.88\%&58.12\%\\
	\hline
	HARP(metapath\&DeepWalk)+word2vec&51.72\%&55.39\%\\
	\hline
	HARP(metapath\&line)+word2vec&60.55\%&53.15\%\\
	\hline
	\hline
	\end{tabular}	
	\caption{Accuracy of models in clustering task.}
	\label{task1cluster}
\end{table}
\subsubsection{Subject Extraction}
We treat the combination of the title and abstract (if any) of a paper as one document. All the documents construct a corpus, then train a LDA model on the corpus with the parameter, number of subjects, equals to 6. Compare the outcome of LDA with the real subjects of the corpus. Outcome  is shown in Table\ref{LDA}.In the table, boldfaced words
  are relatively accurate subjects, the italics are words that relevant
  to this subject but not general enough.
  From table \ref{LDA} we can infer that, as a traditional method for subject extraction, LDA works well for the top 3 subjects. However, the model the last subject should be knowledge discovery in database.
 \subsubsection{Influence Calculation ( In top10Author.xlsx)}
 Use the PageRank index to calculate the influence of the authors, then get the top 8 authors. Compare the score we calculate and the h-index, i10 score google provides. 
 \begin{table}[h]
\center
	\begin{tabular}{c|c|c|c}	
  \hline
  Name & Influence score & h-index & i10 score\\
  \hline
  \hline
  Philip S. Yu & 68.4411 & 142 & 930\\
  \hline
  Jiawei Han & 65.4823 & 159 & 701\\
  \hline
  Michael I. Jordan & 64.1964 & 147 & 463\\
  \hline
  Rakesh Agrawal & 64.0368 & 100 & 264\\
  \hline
  Andrew Y. Ng & 61.737 & 107 & 215\\
  \hline
  Christopher D. Manning & 59.6019 & 103 & 288\\
  \hline
  Wei Wang & 42.5574 & \textbf{117} & \textbf{1299}\\
  \hline
  Christos Faloutsos & 42.126 & \textbf{118} &
  \textbf{451}\\
  \hline
\end{tabular}
\caption{The compareation of the index we calculate and the index google provides.}
  \end{table}
  We can infer that the order of scores is similar. Considering the difference in the scoring method and the inevitable drawbacks that we only use the papers published on the top summits, it is reasonable that there is some difference between the three ratings.
\subsection{Task 2}
\subsection{Task 3}
The data set is divided into training set and test set, as the publish year earlier or later than 2012. Treat a scholar as a node, the co-author relationship as the edges, calculate the preferential attachment score. Sort the prediction edges by the score from high to low, collect the top 16642 edges, which is of  the same size as testing set. There are only 20 predictions that hits those in the testing set, that is to say, the precision, recall and the F1-score are 0.0012. The link prediction precision of the baseline model on the co-author relationship is low mainly due to 2 problems, the method of scoring, the occurrence of existed edges in the prediction.
For this task, we construct several networks. A trivial idea is that we use the whole network, which involves edges between papers and papers(reference), papers and authors, papers and conferences. Another idea is just consider the relationship we want to survey. In the cooperation task, we just construct a network which involves edges between papers and authors. In the citation task, we just construct a network which involves edges between papers and papers, papers and authors. And in publish task, we just construct a network with edges involve relationship between authors and conferences.

To quantify the accuracy of prediction algorithms. We use a method called area under the receiver operating characteristic curve(AUC)\cite{lu2011link}. 
\subsubsection*{Definition of AUC}
Provided the rank of all non-observed links, the AUC values can be interpreted as the probability that a randomly chosen missing link is given a higher score than a randomly chosen nonexistent link. To save time, we calculate the score of each non-observed link instead of giving the ordered list. At each time, we randomly pick a missing link and a nonexistent link to compare their scores. If among n independent comparisons, there are $n_1$ times the missing link having a higher score and $n_2$ times they have the same score, the AUC value is:
\begin{equation}
AUC=\frac{n_1+0.5n_2}{n}
\end{equation}
If all the scores are generated from an independent and identical distribution, the AUC value should be about 0.5. Thereafter, the degree to which the value exceeds 0.5 indicates how much better the algorithm performs than pure chance.

In our experiments, we use the cosine distance between two vectors as the score of two nodes. The result is shown in Table \ref{AUC}
\begin{table}[h]
\center
\begin{tabular}{c|c|c|c}
\hline
\hline
\textbf{Model/task} & \textbf{cooperation} & \textbf{citation} & \textbf{publish}\\
\hline
\hline
metapath2vec++ &75.39\%&66.92\%&77.86\%\\
\hline
HARP(DeepWalk)&83.38\%&\textbf{87.80 \%}&87.76\%\\
\hline
HARP(LINE)&83.73\%&76.93\%& 67.32\%\\
\hline
HARP(DeepWalk)(whole net)& \textbf{85.72\%}&85.30\%&\textbf{88.50\%}\\
\hline
HARP(LINE)(whole net)&56.86\%&56.43\%& 54.81\%\\
\hline
\hline
\end{tabular}
\caption{AUC of task3}
\label{AUC}
\end{table}
\section{Conclusion and Discussion}
\bibliographystyle{IEEEtran}
\bibliography{report}
\end{document}


